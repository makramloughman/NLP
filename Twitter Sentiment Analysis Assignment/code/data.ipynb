{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8e2dedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from twitter_authentication import bearer_token\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2de011f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5ddf815",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "088453ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = tweepy.Client(bearer_token, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3c5c674",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('polls.csv')\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df.set_index('Date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f4cfc217",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets(q, start, end, maximum=10, count=1):\n",
    "    i = 0\n",
    "    objlist = []\n",
    "    for response in tweepy.Paginator(client.search_all_tweets, \n",
    "                                     query = q,\n",
    "                                     user_fields = ['username', 'public_metrics', 'description', 'location'],\n",
    "                                     tweet_fields = ['created_at', 'geo', 'public_metrics', 'text'],\n",
    "                                     expansions = 'author_id',\n",
    "                                     start_time = start,\n",
    "                                     end_time = end,\n",
    "                                     max_results=maximum):\n",
    "        if i == count:\n",
    "            break\n",
    "        i += 1\n",
    "        time.sleep(1.5) # 1 search request per second limit\n",
    "        objlist.append(response)\n",
    "    return objlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b7e28b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_tweets(obj, directory, date):\n",
    "    result = []\n",
    "    user_dict = {}\n",
    "    # Loop through each response object\n",
    "    for response in obj:\n",
    "        # Take all of the users, and put them into a dictionary of dictionaries with the info we want to keep\n",
    "        for user in response.includes['users']:\n",
    "            user_dict[user.id] = {'username': user.username, \n",
    "                                  'followers': user.public_metrics['followers_count'],\n",
    "                                  'tweets': user.public_metrics['tweet_count'],\n",
    "                                  'description': user.description,\n",
    "                                  'location': user.location\n",
    "                                 }\n",
    "        for tweet in response.data:\n",
    "            # For each tweet, find the author's information\n",
    "            author_info = user_dict[tweet.author_id]\n",
    "            # Put all of the information we want to keep in a single dictionary for each tweet\n",
    "            result.append({# 'author_id': tweet.author_id, \n",
    "                           # 'username': author_info['username'],\n",
    "                           'author_followers': author_info['followers'],\n",
    "                           # 'author_tweets': author_info['tweets'],\n",
    "                           # 'author_description': author_info['description'],\n",
    "                           # 'author_location': author_info['location'],\n",
    "                           'text': tweet.text,\n",
    "                           # 'created_at': tweet.created_at,\n",
    "                           'retweets': tweet.public_metrics['retweet_count'],\n",
    "                           'replies': tweet.public_metrics['reply_count'],\n",
    "                           'likes': tweet.public_metrics['like_count'],\n",
    "                           # 'quote_count': tweet.public_metrics['quote_count']\n",
    "                          })\n",
    "\n",
    "    # Change this list of dictionaries into a dataframe\n",
    "    tmp = pd.DataFrame(result)\n",
    "    tmp.to_csv(f\"tweets/{directory}/{date.strftime('%Y-%m-%d')}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "87d32101",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(q, name, pdf=df, maximum=10, count=1):\n",
    "    dates = [df.index[0] + datetime.timedelta(days=-1)] + list(df.index)\n",
    "    for i in tqdm(range(len(dates)-1), desc='Downloading tweets', unit='Date'):\n",
    "        start = dates[i]\n",
    "        end = dates[i+1]\n",
    "        obj = get_tweets(q, start.strftime('%Y-%m-%dT%H:%M:%SZ'), end.strftime('%Y-%m-%dT%H:%M:%SZ'), maximum, count)\n",
    "        time.sleep(1.5)\n",
    "        csv_tweets(obj, name, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dbd4225c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading tweets:  47%|████████████████████████████                                | 43/92 [13:16<22:52, 28.00s/Date]Rate limit exceeded. Sleeping for 97 seconds.\n",
      "Downloading tweets:  82%|████████████████████████████████████████████████▉           | 75/92 [27:53<05:33, 19.64s/Date]Rate limit exceeded. Sleeping for 126 seconds.\n",
      "Downloading tweets: 100%|████████████████████████████████████████████████████████████| 92/92 [37:23<00:00, 24.39s/Date]\n"
     ]
    }
   ],
   "source": [
    "get_data('Fetterman -is:retweet -RT lang:en', 'fetterman', maximum=500, count=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "70562ad3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading tweets:  30%|██████████████████▎                                         | 28/92 [12:24<28:03, 26.31s/Date]Rate limit exceeded. Sleeping for 152 seconds.\n",
      "Downloading tweets:  61%|████████████████████████████████████▌                       | 56/92 [27:24<16:21, 27.25s/Date]Rate limit exceeded. Sleeping for 146 seconds.\n",
      "Downloading tweets:  91%|██████████████████████████████████████████████████████▊     | 84/92 [42:10<03:35, 26.91s/Date]Rate limit exceeded. Sleeping for 167 seconds.\n",
      "Downloading tweets: 100%|████████████████████████████████████████████████████████████| 92/92 [48:29<00:00, 31.63s/Date]\n"
     ]
    }
   ],
   "source": [
    "get_data('Mehmet Oz OR Dr. OZ -is:retweet -RT lang:en', 'oz', maximum=500, count=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
